{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_meeting_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Colab implementation of Roberta_zh for classification\n",
        "\n",
        "To run, first set 'ROOT' to directory containing load_data, model, and utils .py files, as well as the roberta_config configuration file.\n",
        "\n",
        "Within the configuration file, set the data path and inference data path. \n",
        "\n",
        "Finally, alternations are neccesary within the data loader. Ensure that the column names of imported dataframes and the one-hot encoding schemes accord with the data used."
      ],
      "metadata": {
        "id": "ZAM6WfvU6iUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First define root\n",
        "ROOT = 'insert directory containing files'"
      ],
      "metadata": {
        "id": "xol-lEzn6YBC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "wqETQ4rYBBDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "sys.path.append(ROOT)"
      ],
      "metadata": {
        "id": "ug3wQsCkxawG",
        "outputId": "1924877f-d765-460e-85d6-01f75508f174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import py files\n",
        "from load_data import *\n",
        "from model import *\n",
        "from utils import *"
      ],
      "metadata": {
        "id": "i6KH5NucxzRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The experiment function"
      ],
      "metadata": {
        "id": "Mra8Zr-lCUxk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNKGuJojAUEn"
      },
      "outputs": [],
      "source": [
        "from cmath import inf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "class Experiment(object):\n",
        "    def __init__(self, name):\n",
        "\n",
        "        #This pulls in the config data\n",
        "        experiment_config = read_file(ROOT + name + '.json')\n",
        "        if experiment_config is None:\n",
        "            raise Exception(\"Configuration file doesn't exist: \", name)\n",
        "        \n",
        "        #This sets name, to be used when setting directory\n",
        "        self.__name = experiment_config['experiment_name']\n",
        "        \n",
        "        #Sets config\n",
        "        self.__experiment_config = experiment_config\n",
        "        \n",
        "        #Sets dir\n",
        "        self.__experiment_dir = os.path.join(ROOT, self.__name)\n",
        "        \n",
        "        #Used experiment_config to create general model, tokenizer, and specifically configured model\n",
        "        self.__model, self.__tokenizer, self.__config_model = classification_model(experiment_config)\n",
        "\n",
        "        #Get loaders and dfs, third and 4th arg specify the one-hot schema for labels\n",
        "        self.__train_loader, self.__val_loader, self.__test_loader, self.__train_df, self.__val_df, self.__test_df = get_dataset(experiment_config, \n",
        "                                                                                                                                                       self.__tokenizer,\n",
        "                                                                                                                                                ['m', 'T', 'lab', 'temp'],\n",
        "                                                                                                                                                       [0, 1, 1, 1],\n",
        "                                                                                                                                                     'lab')\n",
        "        self.__infer_loader, self.__infer_df = inference_data_processing(experiment_config, self.__tokenizer)                                                                       \n",
        "\n",
        "        #Define params and storage of training output\n",
        "        self.__num_labels = experiment_config['dataset']['num_labels']\n",
        "        self.__epochs = experiment_config['experiment']['num_epochs']\n",
        "        self.__learning_rate = experiment_config['experiment']['learning_rate']\n",
        "        self.__current_epoch = 0\n",
        "        self.__training_losses = []\n",
        "        self.__val_losses = []\n",
        "        self.__best_model = None  # Save your best model in this field and use this in test method.\n",
        "        self.__best_f1 = 0\n",
        "        self.__best_f1_scores = None\n",
        "\n",
        "        #Define cuda\n",
        "        self.__device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        #Define loss function\n",
        "        self.__criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "        #Define learning rate(s)\n",
        "        if isinstance(self.__learning_rate, list):\n",
        "            self.__optimizer = torch.optim.Adam(params=self.__model.parameters(), lr=self.__learning_rate[0])\n",
        "        else:\n",
        "            self.__optimizer = torch.optim.Adam(params=self.__model.parameters(), lr=self.__learning_rate)\n",
        "        self.__init_model()\n",
        "        \n",
        "\n",
        "        #os.makedirs(ROOT_STATS_DIR, exist_ok=True)\n",
        "\n",
        "        #Check for output directory\n",
        "        os.makedirs(self.__experiment_dir, exist_ok=True)\n",
        "\n",
        "    def __init_model(self):\n",
        "        if torch.cuda.is_available():\n",
        "            self.__model = self.__model.cuda().float()\n",
        "            self.__criterion = self.__criterion.cuda()\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        start_epoch = self.__current_epoch\n",
        "        f1_scores = []\n",
        "\n",
        "        tr_labels = []\n",
        "        tr_preds = []\n",
        "        tr_cnkis = []\n",
        "\n",
        "        labels = []\n",
        "        preds = []\n",
        "        cnkis = []\n",
        "\n",
        "        for epoch in range(start_epoch, self.__epochs):  # loop over the dataset multiple times\n",
        "            start_time = datetime.now()\n",
        "            print(f\"### Training epoch: {epoch + 1}\")\n",
        "            \n",
        "            #If list of lr, then use lr according to epoch\n",
        "            if isinstance(self.__learning_rate, list):\n",
        "                for g in self.__optimizer.param_groups: \n",
        "                    g['lr'] = self.__learning_rate[epoch]\n",
        "                lr = self.__optimizer.param_groups[0]['lr']\n",
        "                print(f'### LR = {lr}\\n')\n",
        "            \n",
        "            #Store losses, predictions, and corresponding article IDs\n",
        "            train_loss, train_f1, tr_lab_list, tr_pred_list, tr_cnki_list = self.__train()\n",
        "            val_loss, val_f1, lab_list, pred_list, cnki_list = self.__val()\n",
        "            \n",
        "            labels.append(lab_list)\n",
        "            preds.append(pred_list)\n",
        "            cnkis.append(cnki_list)\n",
        "\n",
        "            tr_labels.append(tr_lab_list)\n",
        "            tr_preds.append(tr_pred_list)\n",
        "            tr_cnkis.append(tr_cnki_list)\n",
        "            \n",
        "            #Save best validation F1 score and best model\n",
        "            f1_scores.append(val_f1)\n",
        "            if val_f1 >= self.__best_f1:\n",
        "                self.__best_f1 = val_f1\n",
        "                self.__best_model = self.__model.state_dict()\n",
        "                self.__save_model(model_path='best_model.pt')                \n",
        "\n",
        "            #Store stats and latest model\n",
        "            self.__record_stats(train_loss, val_loss)\n",
        "            self.__log_epoch_stats(start_time)\n",
        "            self.__save_model()\n",
        "\n",
        "        #Conclude by printing best F1 and list of all F1 scores\n",
        "        print('Training Ended')\n",
        "        print('Best F1:')\n",
        "        print(self.__best_f1)\n",
        "        print('f1 list:')\n",
        "        print(f1_scores)\n",
        "        print()\n",
        "        \n",
        "        # housekeeping\n",
        "        gc.collect() \n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        return labels, preds, cnkis, tr_labels, tr_preds, tr_cnkis, self.__val_df, self.__train_df\n",
        "\n",
        "    def __train(self):\n",
        "\n",
        "        #Set baseline for loss, accuracy, training examples, and steps\n",
        "        #tr_loss, tr_accuracy = 0, 0\n",
        "        tr_loss = 0\n",
        "        nb_tr_examples, nb_tr_steps = 0, 0\n",
        "        \n",
        "        #Create loss and acc list\n",
        "        loss_list = []\n",
        "        pred_list = []\n",
        "        lab_list = []\n",
        "        cnki_list = []\n",
        "        #tr_preds, tr_labels = [], []\n",
        "        \n",
        "        # put model in training mode\n",
        "        self.__model.train()\n",
        "        \n",
        "        for idx, batch in enumerate(self.__train_loader):\n",
        "            \n",
        "            #Move to GPU\n",
        "            ids = batch['input_ids'].to(self.__device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(self.__device, dtype = torch.long)\n",
        "            labels = batch['label'].to(self.__device, dtype = torch.long)\n",
        "            cnkis = batch['cnki_id']\n",
        "\n",
        "            #Return loss and logits from model for batch\n",
        "            loss, tr_logits = self.__model(input_ids=ids, attention_mask=mask, labels=labels, return_dict=False)\n",
        "\n",
        "            #Sum with current loss\n",
        "            tr_loss += loss.item()\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples += labels.size(0)\n",
        "            \n",
        "            #print loss every 200 steps\n",
        "            if idx % 200==0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                # print(f\"Training loss after {idx:04d} training steps: {loss_step}\")\n",
        "                print(f\"Training loss after {idx:04d} training steps: {np.mean(loss_list)}\")\n",
        "\n",
        "            \n",
        "            # compute training accuracy\n",
        "\n",
        "            #Flatten targets and get argmax for logits\n",
        "            flattened_targets = labels.view(-1).cpu().numpy()  # shape (batch_size * seq_len,)\n",
        "            curr_logits = tr_logits.view(-1, self.__model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            predictions = torch.argmax(curr_logits, axis=1).cpu().numpy() \n",
        "\n",
        "            #Acc for batch, then add to overall acc\n",
        "            lab_list.extend(flattened_targets)\n",
        "            #tr_accuracy += tmp_tr_accuracy\n",
        "            pred_list.extend(predictions)\n",
        "            cnki_list.extend(cnkis)\n",
        "            # gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                parameters=self.__model.parameters(), max_norm=self.__experiment_config['model']['max_grad_norm']\n",
        "            )\n",
        "            \n",
        "            # backward pass\n",
        "            self.__optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.__optimizer.step()\n",
        "\n",
        "        #epoch_loss = tr_loss / nb_tr_steps\n",
        "        train_loss = np.mean(loss_list)\n",
        "        # return that loss\n",
        "        #tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "        train_f1 = f1_score(lab_list, pred_list)\n",
        "        # return that acc\n",
        "        print(f\"Training loss epoch: {train_loss}\")\n",
        "        print(f\"Training f1 for epoch: {train_f1}\")\n",
        "        \n",
        "        \n",
        "        return train_loss, train_f1, lab_list, pred_list, cnki_list\n",
        "    \n",
        "    def __val(self):\n",
        "        self.__model.eval()\n",
        "        val_loss = 0\n",
        "        loss_list = []\n",
        "        pred_list = []\n",
        "        label_list = []\n",
        "        cnki_list = []\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, batch in enumerate(self.__val_loader):\n",
        "\n",
        "                #Move to GPU and perform inference\n",
        "                ids = batch['input_ids'].to(self.__device, dtype = torch.long)\n",
        "                mask = batch['attention_mask'].to(self.__device, dtype = torch.long)\n",
        "                labels = batch['label'].to(self.__device, dtype = torch.long)\n",
        "                cnki_ids = batch['cnki_id']\n",
        "\n",
        "                loss, logits = self.__model(ids, attention_mask=mask, labels=labels, return_dict=False)\n",
        "                loss_list.append(loss.item())\n",
        "                flattened_targets = labels.view(-1).cpu().numpy()\n",
        "                all_preds = torch.argmax(logits, axis=-1).cpu().numpy() \n",
        "                label_list.append(flattened_targets)\n",
        "                pred_list.append(all_preds)\n",
        "                cnki_list.append(cnki_ids)\n",
        "\n",
        "        #Flatted\n",
        "        label_list = [x for y in label_list for x in y]\n",
        "        pred_list = [x for y in pred_list for x in y]\n",
        "        cnki_list = [x for y in cnki_list for x in y]\n",
        "\n",
        "        val_f1 = f1_score(label_list, pred_list)\n",
        "        val_loss = np.mean(loss_list)\n",
        "\n",
        "        print(f\"Validation loss epoch: {val_loss}\")\n",
        "        print(f\"Validation f1 for epoch: {val_f1}\")        \n",
        "        return val_loss, val_f1, label_list, pred_list, cnki_list\n",
        "\n",
        "\n",
        "    def test(self, model_loc=None):\n",
        "        self.__best_model = self.__model\n",
        "        if model_loc is not None:\n",
        "            best_checkpoint = torch.load(model_loc)\n",
        "        else:\n",
        "            best_checkpoint = torch.load(os.path.join(self.__experiment_dir, 'best_model.pt'))\n",
        "        self.__best_model.load_state_dict(best_checkpoint['model'])            \n",
        "        self.__best_model = self.__best_model.to(self.__device)\n",
        "        self.__best_model.eval()\n",
        "        test_loss = 0\n",
        "\n",
        "        \n",
        "        loss_list = []\n",
        "        label_list = []\n",
        "        pred_list = []\n",
        "        cnki_list = []\n",
        "        \n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, batch in enumerate(self.__test_loader):\n",
        "\n",
        "                ids = batch['input_ids'].to(self.__device, dtype = torch.long)\n",
        "                mask = batch['attention_mask'].to(self.__device, dtype = torch.long)\n",
        "                labels = batch['label'].to(self.__device, dtype = torch.long)\n",
        "                cnki_ids = batch['cnki_id']\n",
        "\n",
        "                loss, logits = self.__model(ids, attention_mask=mask, labels=labels, return_dict=False)\n",
        "                loss_list.append(loss.item())\n",
        "                flattened_targets = labels.view(-1).cpu().numpy()\n",
        "                all_preds = torch.argmax(logits, axis=-1).cpu().numpy() \n",
        "                label_list.append(flattened_targets)\n",
        "                pred_list.append(all_preds)\n",
        "                cnki_list.append(cnki_ids)\n",
        "\n",
        "        #Flatted\n",
        "        label_list = [x for y in label_list for x in y]\n",
        "        pred_list = [x for y in pred_list for x in y]\n",
        "        cnki_list = [x for y in cnki_list for x in y]\n",
        "\n",
        "        test_f1 = f1_score(label_list, pred_list)\n",
        "        test_loss = np.mean(loss_list)\n",
        "\n",
        "        print(f\"Test loss: {test_loss}\")\n",
        "        print(f\"Test f1: {test_f1}\")        \n",
        "        return test_loss, test_f1, label_list, pred_list, cnki_list\n",
        "\n",
        "    def infer(self, model_loc=None):\n",
        "      self.__best_model = self.__model\n",
        "      if model_loc is not None:\n",
        "          best_checkpoint = torch.load(model_loc)\n",
        "      else:\n",
        "          best_checkpoint = torch.load(os.path.join(self.__experiment_dir, 'best_model.pt'))\n",
        "      self.__best_model.load_state_dict(best_checkpoint['model'])            \n",
        "      self.__best_model = self.__best_model.to(self.__device)\n",
        "      self.__best_model.eval()\n",
        "\n",
        "      loss_list = []\n",
        "      label_list = []\n",
        "      pred_list = []\n",
        "      cnki_list = []\n",
        "    \n",
        "\n",
        "      with torch.no_grad():\n",
        "          for idx, batch in enumerate(self.__infer_loader):\n",
        "\n",
        "              ids = batch['input_ids'].to(self.__device, dtype = torch.long)\n",
        "              mask = batch['attention_mask'].to(self.__device, dtype = torch.long)\n",
        "              cnki_ids = batch['cnki_id']\n",
        "\n",
        "              logits = self.__model(ids, attention_mask=mask, return_dict=False)\n",
        "              all_preds = torch.argmax(logits[0], axis=-1).cpu().numpy() \n",
        "              cnki_list.append(cnki_ids)\n",
        "              pred_list.append(all_preds)\n",
        "\n",
        "      return pred_list, cnki_list\n",
        "\n",
        "    #Auxiliary functions\n",
        "    def __save_model(self, model_path = 'latest_model.pt'):\n",
        "        root_model_path = os.path.join(self.__experiment_dir, model_path)\n",
        "        model_dict = self.__model.state_dict()\n",
        "        state_dict = {'model': model_dict, 'optimizer': self.__optimizer.state_dict()}\n",
        "        torch.save(state_dict, root_model_path)\n",
        "\n",
        "    def __record_stats(self, train_loss, val_loss):\n",
        "        self.__training_losses.append(train_loss)\n",
        "        self.__val_losses.append(val_loss)\n",
        "\n",
        "        self.plot_stats()\n",
        "\n",
        "        write_to_file_in_dir(self.__experiment_dir, 'training_losses.txt', self.__training_losses)\n",
        "        write_to_file_in_dir(self.__experiment_dir, 'val_losses.txt', self.__val_losses)\n",
        "\n",
        "    def __log(self, log_str, file_name=None):\n",
        "        print(log_str)\n",
        "        log_to_file_in_dir(self.__experiment_dir, 'all.log', log_str)\n",
        "        if file_name is not None:\n",
        "            log_to_file_in_dir(self.__experiment_dir, file_name, log_str)\n",
        "\n",
        "    def __log_epoch_stats(self, start_time):\n",
        "        time_elapsed = datetime.now() - start_time\n",
        "        time_to_completion = time_elapsed * (self.__epochs - self.__current_epoch - 1)\n",
        "        train_loss = self.__training_losses[self.__current_epoch]\n",
        "        val_loss = self.__val_losses[self.__current_epoch]\n",
        "        summary_str = \"Epoch: {}, Train Loss: {}, Val Loss: {}, Took {}, ETA: {}\\n\"\n",
        "        summary_str = summary_str.format(self.__current_epoch + 1, train_loss, val_loss, str(time_elapsed),\n",
        "                                         str(time_to_completion))\n",
        "        self.__log(summary_str, 'epoch.log')\n",
        "\n",
        "    def plot_stats(self):\n",
        "        e = len(self.__training_losses)\n",
        "        x_axis = np.arange(1, e + 1, 1)\n",
        "        plt.figure()\n",
        "        plt.plot(x_axis, self.__training_losses, label=\"Training Loss\")\n",
        "        plt.plot(x_axis, self.__val_losses, label=\"Validation Loss\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.legend(loc='best')\n",
        "        plt.title(self.__name + \" Stats Plot\")\n",
        "        plt.savefig(os.path.join(self.__experiment_dir, \"stat_plot.png\"))\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run experiment\n",
        "exp = Experiment('roberta_config')\n",
        "labels, preds, cnkis, tr_labels, tr_preds, tr_cnkis, val_df, tr_df = exp.run()\n",
        "\n",
        "#Return test metrics\n",
        "test_loss, test_f1, label_list, pred_list, cnki_list = exp.test()"
      ],
      "metadata": {
        "id": "k0K3OSimBKMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = Experiment('roberta_config')\n",
        "model_loc = 'BEST MODEL DIR HERE'\n",
        "\n",
        "#Perform inference on new instances\n",
        "inf_pred_list, inf_cnki_list = exp.infer(model_loc)"
      ],
      "metadata": {
        "id": "fubWID0034az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert model output to dataframe\n",
        "all_preds = [y for z in [list(x) for x in inf_pred_list] for y in z]\n",
        "all_cnkis = [x for cnki in inf_cnki_list for x in cnki]\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['cnki_id'] = all_cnkis\n",
        "df['inf_lab'] = all_preds"
      ],
      "metadata": {
        "id": "WNxzhakw8YuP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}